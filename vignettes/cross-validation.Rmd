---
title: "EBMA cross-validation"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The hypothesis this note will hopefully eventually check is whether cross-validation can reduce overfitting in EBMA. Right now it only demonstrates overfitting for the demo example.

```{r}
library("tidyverse")
library("EBMAforecast")
library("EBMAhelper")
```


## Continuous outcome demo: presidential forecasts

The first demo example consists of forecasts for vote share in US presidential elections. I'm using the demo example here, but will use 3 instead of 1 data point as the holdout test set.

To assess performance, I will look at calibration and test period MAE and RMSE for the component models, EBMA, and a baseline simple average ensemble. Relevant questions:

- Does EBMA perform better than the component models?
- Does EBMA perform better than a simple average?
- Overfitting: how much performance decline is there between calibration and test?

Since there is relatively little data, we can use cross-validation to obtain the performance statistics. With 15 rows total and leaving 3 out, there are 455 distinct calibration/test splits. run them all:

```{r}
data(presidentialForecast)

x_data <- presidentialForecast[, c(1:6)]
y_data <- presidentialForecast[, 7]

# 15 choose 3 for train/test split; get all unique combinations
all_splits <- combn(1:15, 3, simplify = FALSE)

output <- list(NULL)
for (i in seq_along(all_splits)) {
  #cat(paste0(i, "."))
  #if (i %% 10 == 0) cat("\n")
  
  # Split 12/3 for calibration/test
  test_idx  <- all_splits[[i]]
  train_idx <- setdiff(1:15, all_splits[[i]])
  
  # Calibrate EBMA
  ebma_fit <- ebma(y = y_data[train_idx], x = x_data[train_idx, ],
                   model_type = "normal", 
                   useModelParams = FALSE, tol = 0.0001, const = 0)
  
  # Save calibration/test RMSE/MAE for each model
  df  <- summary(ebma_fit, period="calibration", showCoefs=FALSE)@summaryData
  out <- data.frame(Period = "calib", Model = rownames(df), df[, c("rmse", "mae")])
  df  <- summary(ebma_fit, period="test", showCoefs=FALSE)@summaryData
  out <- rbind(out, data.frame(Period = "test", Model = rownames(df), df[, c("rmse", "mae")]))
  
  # Just average
  simple_avg <- rowMeans(x_data)
  fit <- data.frame(Period = c("calib", "test"),
                    Model = "Simple avg", 
                    rmse = c(
                      sqrt(mean((simple_avg[train_idx] - y_data[train_idx])^2)),
                      sqrt(mean((simple_avg[test_idx]  - y_data[test_idx])^2))
                    ),
                    mae = c(
                      mean(abs(simple_avg[train_idx] - y_data[train_idx])),
                      mean(abs(simple_avg[test_idx]  - y_data[test_idx]))
                    ))
  
  out <- rbind(out, fit)
  out$index <- i
  
  output[i] <- list(out)
}
output <- do.call(rbind, output)
```

Here are the results, ordered from best test RMSE to worse test RMSE.

```{r}
# Average RMSE/MAE fit by model
output %>%
  tidyr::gather(stat, value, rmse:mae) %>%
  group_by(Model, Period, stat) %>% 
  summarize(mean = mean(value)) %>% 
  ungroup() %>%
  mutate(
    Period = fct_recode(Period, Calib = "calib", Test = "test"),
    stat = toupper(stat)
  ) %>%
  unite(stat, Period, stat) %>%
  spread(stat, mean) %>%
  arrange(Test_RMSE) %>%
  select(Model, ends_with("MAE"), ends_with("RMSE")) %>%
  knitr::kable(digits = 2)
```

- EBMA does better than the input models on both MAE and RMSE and in both the calibration and test sets. 
- A simple average ensemble does better than EBMA, except for head on head in calibration RMSE. 
- Both the MAE and RMSE are about 0.2 higher for the test set than calibration set. Who nows whether that is a lot of overfitting, but in any case it is enough to push EBMA from "competitive with a simple average" in the in-sample predictions to non-competitive in the out-of-sample test predictions. 
